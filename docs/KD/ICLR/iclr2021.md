<h1>ICLR List</h1>
<strong>[ICLR 2021](https://iclr.cc/)</strong>

<ul>
:heavy_check_mark: <li>Undistillable: Making A Nasty Teacher That CANNOT teach students <a href="https://openreview.net/pdf?id=0zvfm-nZqQs">[paper]</a>
<li>Generalization bounds via distillation<a href="https://openreview.net/pdf?id=EGdFhBzmAwB">[paper]</a>
<li>MixKD: Towards Efficient Distillation of Large-scale Language Models<a href="https://openreview.net/pdf?id=UFGEelJkLu5">[paper]</a>
<li>Knowledge distillation via softmax regression representation learning<a href="https://openreview.net/pdf?id=ZzwDy_wiWv">[paper]</a>
<li>Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors<a href="https://openreview.net/pdf?id=uKhGRvM8QNH">[paper]</a>
<li>SEED: Self-supervised Distillation For Visual Representation<a href="https://openreview.net/pdf?id=AHm3dbp7D1D">[paper]</a>
<li>Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks <a href="https://openreview.net/pdf?id=9l0K4OM-oXE">[paper]</a>
<li>istilling Knowledge from Reader to Retriever for Question Answering<a href="https://openreview.net/pdf?id=NTEz-6wysdb">[paper]</a>
<li>Rethinking Soft Labels for Knowledge Distillation: A Biasâ€“Variance Tradeoff Perspective<a href="https://openreview.net/pdf?id=gIHd-5X324">[paper]</a>
<li>Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study<a href="https://openreview.net/pdf?id=PObuuGVrGaZ">[paper]</a>
<li>Knowledge Distillation as Semiparametric Inference<a href="https://openreview.net/pdf?id=m4UCf24r0Y">[paper]</a>
<li>A teacher-student framework to distill future trajectories<a href="https://openreview.net/pdf?id=ECuvULjFQia">[paper]</a>



</ul>


