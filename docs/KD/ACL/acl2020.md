<h1>ACL List</h1>


<strong>[ACL 2020](https://www.aclweb.org/anthology/events/acl-2020/#2020acl-main)</strong>
<ul>
<li>Distilling Knowledge Learned in BERT for Text Generation<a href="https://www.aclweb.org/anthology/2020.acl-main.705.pdf">[paper]</a><a href="https://github.com/ChenRocks/Distill-BERT-Textgen">[code]</a>
<li>Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT<a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.10.pdf">[paper]</a> <a href="https://github.com/castorini/hedwig">[code]</a> 
<li>End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.8.pdf">[paper]</a>
<li>Distilling Neural Networks for Greener and Faster Dependency Parsing<a href="https://www.aclweb.org/anthology/2020.iwpt-1.2.pdf">[paper]</a> <a href="https://github.com/yzhangcs/parser">[Speed Measure code]</a>
<li>XtremeDistil: Multi-stage Distillation for Massive Multilingual Models<a href="https://www.aclweb.org/anthology/2020.acl-main.202.pdf">[paper]</a><a href="https://github.com/MSR-LIT/XtremeDistil/">[code]</a> 
<li>Structure-Level Knowledge Distillation For
Multilingual Sequence Labeling<a href="https://www.aclweb.org/anthology/2020.acl-main.304.pdf">[paper]</a><a href="https://github.com/Alibaba-NLP/MultilangStructureKD">[code]</a> 
<li>Knowledge Distillation for
Multilingual Unsupervised Neural Machine Translation<a href="https://www.aclweb.org/anthology/2020.acl-main.324.pdf">[paper]</a>
<li>FastBERT: a Self-distilling BERT with Adaptive Inference Time<a href="https://www.aclweb.org/anthology/2020.acl-main.537.pdf">[paper]</a><a href="https://github.com/autoliuweijie/FastBERT">[code]</a>
<li>TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural
Language Processing<a href="https://www.aclweb.org/anthology/2020.acl-demos.2.pdf">[paper]</a><a href="https://github.com/airaria/TextBrewer">[code]</a>
<li>Distill, Adapt, Distill:
Training Small, In-Domain Models for Neural Machine Translation<a href="https://www.aclweb.org/anthology/2020.ngt-1.12.pdf">[paper]</a><a href="https://github.com/mitchellgordon95/kd-adapt">[code]</a>

</ul>

