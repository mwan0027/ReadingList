<h1>Knowledge Distillation Survey Paper_List</h1>

## Different form of Knowledge

### Response-Based Knowledge
<ul>
<li> <strong>Do deep nets really need to be deep?</strong> Ba, J., & Caruana, R. (2014). NIPS <a href="https://papers.nips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf">[paper]</a></li> 


<li> <strong>Distilling the knowledge in a neural network.</strong> Hinton, G., Vinyals, O., & Dean, J. (2015). <a href="http://www.cs.toronto.edu/~hinton/absps/distillation.pdf">[paper]</a></li>

<li><strong>Transferring
knowledge to smaller network with class-distance
loss.</strong> Kim, S. W. & Kim, H. E. (2017). In: ICLRW. <a href="https://openreview.net/pdf?id=ByXrfaGFe">[paper]</a></li>

<li><strong>Improved knowledge distillation via teacher assistant.</strong>Mirzadeh, S. I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., & Ghasemzadeh, H. (2020, April). AAAI. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5963">[paper]</a></li>

<li><strong>Learning efficient object detection models with knowledge distillation.</strong>Chen, G., Choi, W., Yu, X., Han, T., & Chandraker, M. (2017). In: NeurIPS. <a href="https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf">[paper]</a></li>

<li><strong>Conditional teacher-student learning.</strong>Meng, Z., Li, J., Zhao, Y., & Gong, Y. (2019, May). In: ICASSP. <a href="https://arxiv.org/pdf/1904.12399">[paper]</a></li>

<li><strong>Feature fusion for online mutual knowledge distillation.</strong>Kim, J., Hyun, M., Chung, I., & Kwak, N. (2019). In: ICPR. <a href="https://arxiv.org/pdf/1904.09058.pdf">[paper]</a></li>

<li><strong>When does label smoothing help?</strong>MÃ¼ller, R., Kornblith, S., & Hinton, G. E. (2019). In: NeurIPS. <a href="https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf">[paper]</a></li>

</ul>

### Feature-Based Knowledge

<li><strong>WFitnets: Hints for thin deep nets.</strong>Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2014). In: ICLR. <a href="https://arxiv.org/pdf/1412.6550.pdf?source=post_page---------------------------">[paper]</a></li>

<li><strong>Paying more
attention to attention: Improving the performance of
convolutional neural networks via attention transfer.</strong>Zagoruyko, S., & Komodakis, N. (2016).  In: ICLR. <a href="https://arxiv.org/pdf/1612.03928.pdf">[paper]</a></li>

<li><strong>Spatiotemporal distilled dense-connectivity network for video action recognition.</strong>Hao, W., & Zhang, Z. (2019). Pattern Recognition. <a href="http://159.226.21.68/bitstream/173211/23347/1/Spatiotemporal%20distilled%20denseConnectivity%20network%20for%20video%20action%20recognition.pdf">[paper]</a></li>

<li><strong>Learning deep representations with probabilistic knowledge transfer.</strong>Passalis, N. & Tefas, A. (2018).  In: ECCV. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.pdf">[paper]</a></li>

<li><strong>Paraphrasing complex network: Network compression via factor transfer.</strong> Kim, J., Park, S., & Kwak, N. (2018). In: NeurIPS <a href="https://proceedings.neurips.cc/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation via
route constrained optimization. </strong>Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D.,
Yan, J. & Hu, X. (2019). In: ICCV. <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Jin_Knowledge_Distillation_via_Route_Constrained_Optimization_ICCV_2019_paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation with adversarial samples
supporting decision boundary.</strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4263/4141">[paper]</a></li>

<li><strong>Rocket launching: A universal and efficient framework for training well-performing light net.</strong>Zhou, G., Fan, Y., Cui, R., Bian, W., Zhu, X., & Gai, K. (2017).  <a href="https://arxiv.org/pdf/1708.04106">[paper]</a></li>

<li><strong>Differentiable Feature Aggregation Search for Knowledge Distillation.</strong>Guan, Y., Zhao, P., Wang, B., Zhang, Y., Yao, C., Bian, K., & Tang, J. (2020, August). In: ECCV <a href="https://arxiv.org/pdf/2008.00506">[paper]</a></li>

<li><strong> Amalgamating knowledge towards comprehensive classification</strong>Shen, C., Wang, X., Song, J., Sun, L., & Song, M.
(2019). In: AAAI. <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4165/4043">[paper]</a></li>

### Relation-Based Knowledge

<li><strong></strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="">[paper]</a></li>

### - Relationships between Intermediate layers

### - Parameters of teacher model

## Different mode of distillation

## KD algorithms

## KD Applications

<ul>
<li> Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN
</ul>