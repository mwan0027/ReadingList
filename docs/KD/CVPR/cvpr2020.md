<strong>[CVPR 2020](https://openaccess.thecvf.com/CVPR2020_search)</strong>
 
 <ul>
 <li>Spatio-Temporal Graph for Video Captioning With Knowledge Distillation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>
 <li>Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yin_Dreaming_to_Distill_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Search to Distill: Pearls Are Everywhere but Not the Eyes <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Search_to_Distill_Pearls_Are_Everywhere_but_Not_the_Eyes_CVPR_2020_paper.pdf">[paper]</a>
 <li>Explaining Knowledge Distillation by Quantifying the Knowledge<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf">[paper]</a>
 <li>Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Networks_Are_More_Productive_Teachers_Than_Human_Raters_Active_CVPR_2020_paper.pdf">[paper]</a> <a href="https://www.youtube.com/watch?v=yBO8olcWHvE">[video]</a> 
 <li>Regularizing Class-Wise Predictions via Self-Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yun_Regularizing_Class-Wise_Predictions_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Creating_Something_From_Nothing_Unsupervised_Knowledge_Distillation_for_Cross-Modal_Hashing_CVPR_2020_paper.pdf">[paper]</a><a href="https://www.youtube.com/watch?v=Io1uloVOEJk">[video]</a> 
 <li>Heterogeneous Knowledge Distillation Using Information Flow Modeling<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Passalis_Heterogeneous_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Revisiting Knowledge Distillation via Label Smoothing Regularization<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yuan_Revisiting_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Collaborative Distillation for Ultra-Resolution Universal Style Transfer<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.pdf">[paper]</a>
 <li>Few Sample Knowledge Distillation for Efficient Network Compression<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Few_Sample_Knowledge_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Online Knowledge Distillation via Collaborative Learning<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf">[paper]</a>
 <li>Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Block-Wisely_Supervised_Neural_Architecture_Search_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Block-Wisely_Supervised_Neural_CVPR_2020_supplemental.pdf">[supp]</a> <a href="https://www.youtube.com/watch?v=DCBYhdCPpoA">[video]</a> 
 </ul>

 ## TODO


<!-- #SIGIR -->
 <strong>[SIGIR 2020](http://static.ijcai.org/2020-accepted_papers.html)</strong>
<ul>
<li>Differentially Private Knowledge Distillation for Mobile Analytics [short paper] <a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401259"> [page]</a>
<li>Distilling Knowledge for fast retrieval-based chat-bots [short paper]<a href="https://arxiv.org/pdf/2004.11045.pdf">[paper]</a>
<li>A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data <a href="http://csse.szu.edu.cn/staff/panwk/publications/Conference-SIGIR-20-KDCRec.pdf">[paper]</a>
<a href="https://github.com/dgliu/SIGIR20_KDCRec.">[code]</a>
</ul>


<!-- #IJCAI -->
 <strong>[IJCAI 2020](http://static.ijcai.org/2020-accepted_papers.html)</strong>
<ul>
<li>P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection <a href="https://www.ijcai.org/Proceedings/2020/0448.pdf">[paper]</a>
</ul>

<!-- #ECCV -->
 <strong>[ECCV 2020](https://eccv2020.eu/)</strong>
<ul>
<li>Circumventing Outliers of AutoAugment with Knowledge Distillation<a href="">[paper]</a>
<li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480613.pdf">[paper]</a>
<li>Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500239.pdf">[paper]</a>
<li>Knowledge Distillation Meets Self-Supervision<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562-supp.pdf">[supp]</a>
<li>Local Correlation Consistency for Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018-supp.zip">[supp]</a>
<li>Robust Re-Identification by Multiple Views Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550103.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550103-supp.pdf">[supp]</a>
<li>AMLN: Adversarial-based Mutual Learning Network for Online Knowledge Distilla<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570154.pdf">[paper]</a>
<li>Differentiable Feature Aggregation Search for Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620460.pdf">[paper]</a>
<li>Online Ensemble Model Compression using Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640018.pdf">[paper]</a>
<li>Exclusivity-Consistency Regularized Knowledge Distillation for Face Recognition<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690324.pdf">[paper]</a>
<li>Feature Normalized Knowledge Distillation for Image Classification<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700664.pdf">[paper]</a>
<li>Weight Decay Scheduling and Knowledge Distillation for Active Learning<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710426.pdf">[paper]</a>
<li>Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710647.pdf">[paper]</a><a href="hhttps://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710647-supp.pdf">[supp]</a>
<li>Interpretable Foreground Object Search As Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730188.pdf">[paper]</a>
<li>Improving Knowledge Distillation via Category Structure<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730205.pdf">[paper]</a>
<li>Circumventing Outliers of AutoAugment with Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480613.pdf">[paper]</a>
</ul>
