<h1>AAAI List</h1>

<strong>[AAAI 2021](https://aaai.org/Conferences/AAAI-21/)</strong>
<ul>
<li>PSSM-Distil: Protein Secondary Structure Prediction (PSSP) on Low-Quality PSSM by Knowledge Distillation with Contrastive Learning. <a href="">[paper_ToBeUpdate]</a>
<li>Probing Product Description Generation via Posterior Distillation <a href="">[paper_ToBeUpdate]</a>
<li>LRC-BERT: Latent-Representation Contrastive Knowledge Distillation for Natural Language Understanding<a href="https://arxiv.org/pdf/2012.07335">[paper]</a>
<li>Peer Collaborative Learning for Online Knowledge Distillation<a href="https://arxiv.org/pdf/2006.04147">[paper]</a>
<li>Cross-Layer Distillation with Semantic Calibration<a href="https://arxiv.org/pdf/2012.03236">[paper]</a>
<li>Few-Shot Class-Incremental Learning via Relation Knowledge Distillation<a href="">[paper_ToBeUpdate]</a>
<li> Harmonized Dense Knowledge Distillation Training for Multi-Exit Architectures<a href="">[paper_ToBeUpdate]</a>
<li>Universal Trading for Order Execution with Oracle Policy Distillation<a href="">[paper_ToBeUpdate]</a>
<li>Diverse Knowledge Distillation for End-to-End Person Search<a href="https://arxiv.org/pdf/2012.11187">[paper]</a>
<li>Distilling Localization for Self-Supervised Representation Learning<a href="https://arxiv.org/pdf/2004.06638">[paper]</a>
<li>Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation<a href="https://arxiv.org/pdf/2101.08106">[paper]</a>
<li>Multi-View Feature Representation for Dialogue Generation with Bidirectional Distillation<a href="https://jklj077.github.io/publication/aaai2021-multiview/">[paper_ToBeUpdate]</a>
<li> Data-Free Knowledge Distillation with Soft Targeted Transfer Set Synthesis<a href="">[paper_ToBeUpdate]</a>
<li> ALP-KD: Attention-Based Layer Projection for Knowledge Distillation<a href="https://arxiv.org/pdf/2012.14022">[paper]</a>
<li>Progressive Network Grafting for Few-Shot Knowledge Distillation<a href="https://arxiv.org/pdf/2012.04915">[paper]</a>
<li> Show, Attend and Distill: Knowledge Distillation via Attention-Based Feature Matching<a href="https://arxiv.org/pdf/2102.02973">[paper]</a>
<li>Anytime Inference with Distilled Hierarchical Neural Ensembles<a href="">[paper_ToBeUpdate]</a>
<li>Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks<a href="https://arxiv.org/pdf/2009.14502">[paper]</a>
<li>Reinforced Multi-Teacher Selection for Knowledge Distillation<a href="https://arxiv.org/pdf/2012.06048">[paper]</a>
<li> Deep Contextual Clinical Prediction with Reverse Distillation<a href="https://arxiv.org/pdf/2007.05611">[paper]</a>

</ul>