# ReadingList



## Conferences
 [CVPR 2020](https://openaccess.thecvf.com/CVPR2020_search)
 


# Knowledge Distillation

## Different form of Knowledge

### - Logits of large deep model
<ul>
<li> <strong>Do deep nets really need to be deep?</strong> Ba, J., & Caruana, R. (2014). NIPS <a href="https://papers.nips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf">[paper]</a></li> 


<li> <strong>Distilling the knowledge in a neural network.</strong> Hinton, G., Vinyals, O., & Dean, J. (2015). <a href="http://www.cs.toronto.edu/~hinton/absps/distillation.pdf">[paper]</a></li>

<li><strong>Paraphrasing complex network: Network compression via factor transfer.</strong> Kim, J., Park, S., & Kwak, N. (2018). In: NeurIPS <a href="https://proceedings.neurips.cc/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf">[paper]</a></li>

<li><strong>Improved knowledge distillation via teacher assistant.</strong>Mirzadeh, S. I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., & Ghasemzadeh, H. (2020, April). AAAI. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5963">[paper]</a></li>
 
</ul>

### - Activation, neurons or features of Intermediate layers


### - Relationships between Intermediate layers

### - Parameters of teacher model

## Different mode of distillation

## KD algorithms

## KD Applications

<ul>
<li> Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN
</ul>