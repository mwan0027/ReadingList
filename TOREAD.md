# To-Do: ReadingList


## Conferences
<!-- CVPR -->
 [CVPR 2020](https://openaccess.thecvf.com/CVPR2020_search)
 
 <ul>
 <li>Spatio-Temporal Graph for Video Captioning With Knowledge Distillation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>
 <li>Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yin_Dreaming_to_Distill_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Search to Distill: Pearls Are Everywhere but Not the Eyes <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Search_to_Distill_Pearls_Are_Everywhere_but_Not_the_Eyes_CVPR_2020_paper.pdf">[paper]</a>
 <li>Explaining Knowledge Distillation by Quantifying the Knowledge<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf">[paper]</a>
 <li>Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Networks_Are_More_Productive_Teachers_Than_Human_Raters_Active_CVPR_2020_paper.pdf">[paper]</a> <a href="https://www.youtube.com/watch?v=yBO8olcWHvE">[video]</a> 
 <li>Regularizing Class-Wise Predictions via Self-Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yun_Regularizing_Class-Wise_Predictions_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Creating_Something_From_Nothing_Unsupervised_Knowledge_Distillation_for_Cross-Modal_Hashing_CVPR_2020_paper.pdf">[paper]</a><a href="https://www.youtube.com/watch?v=Io1uloVOEJk">[video]</a> 
 <li>Heterogeneous Knowledge Distillation Using Information Flow Modeling<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Passalis_Heterogeneous_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Revisiting Knowledge Distillation via Label Smoothing Regularization<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yuan_Revisiting_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Collaborative Distillation for Ultra-Resolution Universal Style Transfer<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.pdf">[paper]</a>
 <li>Few Sample Knowledge Distillation for Efficient Network Compression<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Few_Sample_Knowledge_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Online Knowledge Distillation via Collaborative Learning<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf">[paper]</a>
 <li>Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Block-Wisely_Supervised_Neural_Architecture_Search_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Block-Wisely_Supervised_Neural_CVPR_2020_supplemental.pdf">[supp]</a> <a href="https://www.youtube.com/watch?v=DCBYhdCPpoA">[video]</a> 
 </ul>



<!-- #ICLR -->
 [ICLR 2020](https://iclr.cc/virtual_2020/papers.html?filter=keywords&search=knowledge+distillation)

<ul>
<li>Neural Epitome Search for Architecture-Agnostic Network Compression <a href="https://openreview.net/pdf?id=HyxjOyrKvr">[paper]</a>
<li>Contrastive Representation Distillation<a href="http://www.openreview.net/pdf?id=SkgpBJrtvS">[paper]</a><a href="https://github.com/HobbitLong/RepDistiller">[code]</a>
<li>Ensemble Distribution Distillation<a href="http://www.openreview.net/pdf?id=BygSP6Vtvr">[paper]</a>
<li>Understanding Knowledge Distillation in Non-autoregressive Machine Translation<a href="http://www.openreview.net/pdf?id=BygFVAEKDH">[paper]</a>
<li>Neural Epitome Search for Architecture-Agnostic Network Compression<a href="http://www.openreview.net/pdf?id=HyxjOyrKvr">[paper]</a>

</ul>



# Knowledge Distillation

## Different form of Knowledge

### Response-Based Knowledge
<ul>
<li> <strong>Do deep nets really need to be deep?</strong> Ba, J., & Caruana, R. (2014). NIPS <a href="https://papers.nips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf">[paper]</a></li> 


<li> <strong>Distilling the knowledge in a neural network.</strong> Hinton, G., Vinyals, O., & Dean, J. (2015). <a href="http://www.cs.toronto.edu/~hinton/absps/distillation.pdf">[paper]</a></li>

<li><strong>Transferring
knowledge to smaller network with class-distance
loss.</strong> Kim, S. W. & Kim, H. E. (2017). In: ICLRW. <a href="https://openreview.net/pdf?id=ByXrfaGFe">[paper]</a></li>

<li><strong>Improved knowledge distillation via teacher assistant.</strong>Mirzadeh, S. I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., & Ghasemzadeh, H. (2020, April). AAAI. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5963">[paper]</a></li>

<li><strong>Learning efficient object detection models with knowledge distillation.</strong>Chen, G., Choi, W., Yu, X., Han, T., & Chandraker, M. (2017). In: NeurIPS. <a href="https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf">[paper]</a></li>

<li><strong>Conditional teacher-student learning.</strong>Meng, Z., Li, J., Zhao, Y., & Gong, Y. (2019, May). In: ICASSP. <a href="https://arxiv.org/pdf/1904.12399">[paper]</a></li>

<li><strong>Feature fusion for online mutual knowledge distillation.</strong>Kim, J., Hyun, M., Chung, I., & Kwak, N. (2019). In: ICPR. <a href="https://arxiv.org/pdf/1904.09058.pdf">[paper]</a></li>

<li><strong>When does label smoothing help?</strong>MÃ¼ller, R., Kornblith, S., & Hinton, G. E. (2019). In: NeurIPS. <a href="https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf">[paper]</a></li>

</ul>

### Feature-Based Knowledge

<li><strong>WFitnets: Hints for thin deep nets.</strong>Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2014). In: ICLR. <a href="https://arxiv.org/pdf/1412.6550.pdf?source=post_page---------------------------">[paper]</a></li>

<li><strong>Paying more
attention to attention: Improving the performance of
convolutional neural networks via attention transfer.</strong>Zagoruyko, S., & Komodakis, N. (2016).  In: ICLR. <a href="https://arxiv.org/pdf/1612.03928.pdf">[paper]</a></li>

<li><strong>Spatiotemporal distilled dense-connectivity network for video action recognition.</strong>Hao, W., & Zhang, Z. (2019). Pattern Recognition. <a href="http://159.226.21.68/bitstream/173211/23347/1/Spatiotemporal%20distilled%20denseConnectivity%20network%20for%20video%20action%20recognition.pdf">[paper]</a></li>

<li><strong>Learning deep representations with probabilistic knowledge transfer.</strong>Passalis, N. & Tefas, A. (2018).  In: ECCV. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.pdf">[paper]</a></li>

<li><strong>Paraphrasing complex network: Network compression via factor transfer.</strong> Kim, J., Park, S., & Kwak, N. (2018). In: NeurIPS <a href="https://proceedings.neurips.cc/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation via
route constrained optimization. </strong>Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D.,
Yan, J. & Hu, X. (2019). In: ICCV. <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Jin_Knowledge_Distillation_via_Route_Constrained_Optimization_ICCV_2019_paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation with adversarial samples
supporting decision boundary.</strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4263/4141">[paper]</a></li>

<li><strong>Rocket launching: A universal and efficient framework for training well-performing light net.</strong>Zhou, G., Fan, Y., Cui, R., Bian, W., Zhu, X., & Gai, K. (2017).  <a href="https://arxiv.org/pdf/1708.04106">[paper]</a></li>

<li><strong>Differentiable Feature Aggregation Search for Knowledge Distillation.</strong>Guan, Y., Zhao, P., Wang, B., Zhang, Y., Yao, C., Bian, K., & Tang, J. (2020, August). In: ECCV <a href="https://arxiv.org/pdf/2008.00506">[paper]</a></li>

<li><strong> Amalgamating knowledge towards comprehensive classification</strong>Shen, C., Wang, X., Song, J., Sun, L., & Song, M.
(2019). In: AAAI. <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4165/4043">[paper]</a></li>

### Relation-Based Knowledge

<li><strong></strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="">[paper]</a></li>

### - Relationships between Intermediate layers

### - Parameters of teacher model

## Different mode of distillation

## KD algorithms

## KD Applications

<ul>
<li> Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN
</ul>