# To-Do: ReadingList

Reading List of recent published Knowledge-Distillation papers

## Knowledge Distillation
<!-- CVPR -->
 <strong>[CVPR 2020](https://openaccess.thecvf.com/CVPR2020_search)</strong>
 
 <ul>
 <li>Spatio-Temporal Graph for Video Captioning With Knowledge Distillation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>
 <li>Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yin_Dreaming_to_Distill_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Search to Distill: Pearls Are Everywhere but Not the Eyes <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Search_to_Distill_Pearls_Are_Everywhere_but_Not_the_Eyes_CVPR_2020_paper.pdf">[paper]</a>
 <li>Explaining Knowledge Distillation by Quantifying the Knowledge<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf">[paper]</a>
 <li>Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Networks_Are_More_Productive_Teachers_Than_Human_Raters_Active_CVPR_2020_paper.pdf">[paper]</a> <a href="https://www.youtube.com/watch?v=yBO8olcWHvE">[video]</a> 
 <li>Regularizing Class-Wise Predictions via Self-Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yun_Regularizing_Class-Wise_Predictions_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Creating_Something_From_Nothing_Unsupervised_Knowledge_Distillation_for_Cross-Modal_Hashing_CVPR_2020_paper.pdf">[paper]</a><a href="https://www.youtube.com/watch?v=Io1uloVOEJk">[video]</a> 
 <li>Heterogeneous Knowledge Distillation Using Information Flow Modeling<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Passalis_Heterogeneous_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Revisiting Knowledge Distillation via Label Smoothing Regularization<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yuan_Revisiting_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Collaborative Distillation for Ultra-Resolution Universal Style Transfer<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.pdf">[paper]</a>
 <li>Few Sample Knowledge Distillation for Efficient Network Compression<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Few_Sample_Knowledge_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Online Knowledge Distillation via Collaborative Learning<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf">[paper]</a>
 <li>Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Block-Wisely_Supervised_Neural_Architecture_Search_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Block-Wisely_Supervised_Neural_CVPR_2020_supplemental.pdf">[supp]</a> <a href="https://www.youtube.com/watch?v=DCBYhdCPpoA">[video]</a> 
 </ul>


<!-- #ACL -->
 <strong>[ACL 2020](https://www.aclweb.org/anthology/events/acl-2020/#2020acl-main)</strong>
<ul>
<li>Distilling Knowledge Learned in BERT for Text Generation<a href="https://www.aclweb.org/anthology/2020.acl-main.705.pdf">[paper]</a><a href="https://github.com/ChenRocks/Distill-BERT-Textgen">[code]</a>
<li>Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT<a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.10.pdf">[paper]</a> <a href="https://github.com/castorini/hedwig">[code]</a> 
<li>End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.8.pdf">[paper]</a>
<li>Distilling Neural Networks for Greener and Faster Dependency Parsing<a href="https://www.aclweb.org/anthology/2020.iwpt-1.2.pdf">[paper]</a> <a href="https://github.com/yzhangcs/parser">[Speed Measure code]</a>
<li>XtremeDistil: Multi-stage Distillation for Massive Multilingual Models<a href="https://www.aclweb.org/anthology/2020.acl-main.202.pdf">[paper]</a><a href="https://github.com/MSR-LIT/XtremeDistil/">[code]</a> 
<li>Structure-Level Knowledge Distillation For
Multilingual Sequence Labeling<a href="https://www.aclweb.org/anthology/2020.acl-main.304.pdf">[paper]</a><a href="https://github.com/Alibaba-NLP/MultilangStructureKD">[code]</a> 
<li>Knowledge Distillation for
Multilingual Unsupervised Neural Machine Translation<a href="https://www.aclweb.org/anthology/2020.acl-main.324.pdf">[paper]</a>
<li>FastBERT: a Self-distilling BERT with Adaptive Inference Time<a href="https://www.aclweb.org/anthology/2020.acl-main.537.pdf">[paper]</a><a href="https://github.com/autoliuweijie/FastBERT">[code]</a>
<li>TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural
Language Processing<a href="https://www.aclweb.org/anthology/2020.acl-demos.2.pdf">[paper]</a><a href="https://github.com/airaria/TextBrewer">[code]</a>
<li>Distill, Adapt, Distill:
Training Small, In-Domain Models for Neural Machine Translation<a href="https://www.aclweb.org/anthology/2020.ngt-1.12.pdf">[paper]</a><a href="https://github.com/mitchellgordon95/kd-adapt">[code]</a>

</ul>

<!-- #ICLR -->
 <strong>[ICLR 2020](https://iclr.cc/virtual_2020/papers.html?filter=keywords&search=knowledge+distillation)</strong>

<ul>
<li>Neural Epitome Search for Architecture-Agnostic Network Compression <a href="https://openreview.net/pdf?id=HyxjOyrKvr">[paper]</a>
<li>Contrastive Representation Distillation<a href="http://www.openreview.net/pdf?id=SkgpBJrtvS">[paper]</a><a href="https://github.com/HobbitLong/RepDistiller">[code]</a>
<li>Ensemble Distribution Distillation<a href="http://www.openreview.net/pdf?id=BygSP6Vtvr">[paper]</a>
<li>Understanding Knowledge Distillation in Non-autoregressive Machine Translation<a href="http://www.openreview.net/pdf?id=BygFVAEKDH">[paper]</a>
<li>Neural Epitome Search for Architecture-Agnostic Network Compression<a href="http://www.openreview.net/pdf?id=HyxjOyrKvr">[paper]</a>

</ul>


<!-- #EMNLP -->
 <strong>[EMNLP 2020](https://2020.emnlp.org/papers/main)</strong>
<ul>
<li>FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.165.pdf">[paper]</a>
<li>Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. <a href="https://arxiv.org/pdf/2004.09813">[paper]</a>
<li>Adversarial Self-Supervised Data-Free Distillation for Text Classification<a href="https://www.aclweb.org/anthology/2020.emnlp-main.499.pdf">[paper]</a>
<li>Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data<a href="https://www.aclweb.org/anthology/2020.emnlp-main.277.pdf">[paper]</a>
<li>Lifelong Language Knowledge Distillation.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.233.pdf">[paper]</a>
<li>Autoregressive Knowledge Distillation through Imitation Learning.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.494.pdf">[paper]</a>
<li>TernaryBERT: Distillation-aware Ultra-low Bit BERT.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.37.pdf">[paper]</a>
<li>Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast---Choose Three. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.450.pdf">[paper]</a>
<li>Improving Neural Topic Models using Knowledge Distillation. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.137.pdf">[paper]</a>
<li>Contrastive Distillation on Intermediate Representations for Language Model Compression.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.36.pdf">[paper]</a>
<li>Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers. [short paper] <a href="https://www.aclweb.org/anthology/2020.emnlp-main.74.pdf">[paper]</a>
</ul>


<!-- #AAAI -->
 <strong>[AAAI 2020]()</strong>
<ul>
<li>Uncertainty-aware Multi-shot Knowledge Distillation for Image-based Object Re-identification. <a href="https://www.msra.cn/wp-content/uploads/2020/01/Uncertainty-aware-Multi-shot-Knowledge-Distillation-for-Image-based-Object-Re-identification.pdf">[paper]</a>
<li>Towards Cross-modality Medical Image Segmentation with Online Mutual Knowledge Distillation. <a href="https://www.researchgate.net/profile/Lequan_Yu/publication/341886190_Towards_Cross-Modality_Medical_Image_Segmentation_with_Online_Mutual_Knowledge_Distillation/links/5edcb14292851c9c5e8b0b51/Towards-Cross-Modality-Medical-Image-Segmentation-with-Online-Mutual-Knowledge-Distillation.pdf">[paper]</a>
<li>Ultrafast Video Attention Prediction with Coupled Knowledge Distillation. <a href="https://arxiv.org/pdf/1904.04449">[paper]</a>
<li>Online Knowledge Distillation with Diverse Peers. <a href="https://www.researchgate.net/profile/Jian-Ping_Mei/publication/337413771_Online_Knowledge_Distillation_with_Diverse_Peers/links/5dd60feb458515cd48aff978/Online-Knowledge-Distillation-with-Diverse-Peers.pdf">[paper]</a>
<li>Towards Oracle Knowledge Distillation with Neural Architecture Search. <a href="https://www.researchgate.net/profile/Lequan_Yu/publication/341886190_Towards_Cross-Modality_Medical_Image_Segmentation_with_Online_Mutual_Knowledge_Distillation/links/5edcb14292851c9c5e8b0b51/Towards-Cross-Modality-Medical-Image-Segmentation-with-Online-Mutual-Knowledge-Distillation.pdf">[paper]</a>
<li>Knowledge Distillation from Internal Representations. <a href="https://assets.amazon.science/7a/a6/7e6c8be54bd4a9674ba3826988e3/knowledge-distillation-from-internal-rpresentations.pdf">[paper]</a>
<li>Improved Knowledge Distillation via Teacher Assistant. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/5963/5819">[paper]</a> <a href="https://imirzadeh.me/dl/TAKD_poster.pdf">[poster]</a>
<li>Distilling portable Generative Adversarial Networks for Image Translation. <a href="https://arxiv.org/pdf/2003.03519.pdf">[paper]</a>
<li>Look One and More: Distilling Hybrid Order Relational Knowledge for Cross-Resolution Image Recognition. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6715/6569">[paper]</a>
<li>Learning end-to-end scene flow by distilling single tasks knowledge. <a href="https://www.researchgate.net/profile/Stefano_Mattoccia/publication/337484940_Learning_End-To-End_Scene_Flow_by_Distilling_Single_Tasks_Knowledge/links/5ddb79eda6fdccdb4462c695/Learning-End-To-End-Scene-Flow-by-Distilling-Single-Tasks-Knowledge.pdf">[paper]</a>
<li>Few shot network compression via cross distillation. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5718/5574">[paper]</a>
<li>Adversarially Robust Distillation. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/5816/5672">[paper]</a>
<li>Multi-source Distilling Domain Adaptation. [TBD]<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6997/6851">[paper]</a>
<li>Long Short-Term Sample Distillation. <a href="https://arxiv.org/pdf/2003.00739.pdf">[paper]</a>
<li> Scalable Attentive Sentence-Pair Modeling via Distilled Sentence Embedding. <a href="https://arxiv.org/pdf/1908.05161.pdf">[paper]</a>
<li>Distilling Knowledge from Well-informed Soft Labels for Neural Relation Extraction. <a href="https://arxiv.org/pdf/1908.05161">[paper]</a>
<li>Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers. <a href="https://www.researchgate.net/profile/Xinchao_Wang/publication/337560210_Hearing_Lips_Improving_Lip_Reading_by_Distilling_Speech_Recognizers/links/5e2fc35992851c9af729e6fe/Hearing-Lips-Improving-Lip-Reading-by-Distilling-Speech-Recognizers.pdf">[paper]</a>
<li>Filtration and Distillation: Enhancing Region Attention for Fine-Grained Visual Categorization. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/6822/6676">[paper]</a>
</ul>


<!-- #SIGIR -->
 <strong>[SIGIR 2020](http://static.ijcai.org/2020-accepted_papers.html)</strong>
<ul>
<li>Differentially Private Knowledge Distillation for Mobile Analytics [short paper] <a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401259"> [page]</a>
<li>Distilling Knowledge for fast retrieval-based chat-bots [short paper]<a href="https://arxiv.org/pdf/2004.11045.pdf">[paper]</a>
<li>A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data <a href="http://csse.szu.edu.cn/staff/panwk/publications/Conference-SIGIR-20-KDCRec.pdf">[paper]</a>
<a href="https://github.com/dgliu/SIGIR20_KDCRec.">[code]</a>
</ul>


<!-- #IJCAI -->
 <strong>[IJCAI 2020](http://static.ijcai.org/2020-accepted_papers.html)</strong>
<ul>
<li>P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection <a href="https://www.ijcai.org/Proceedings/2020/0448.pdf">[paper]</a>
</ul>

## Federated Learning

<!-- #IEEE -->
 <strong>[IEEE ICKG 2020](http://ickg2020.bigke.org/)</strong>
<ul>
<li>Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9194566">[paper]</a>
</ul>


# To-Do: Knowledge Distillation Survey Paper


## Different form of Knowledge

### Response-Based Knowledge
<ul>
<li> <strong>Do deep nets really need to be deep?</strong> Ba, J., & Caruana, R. (2014). NIPS <a href="https://papers.nips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf">[paper]</a></li> 


<li> <strong>Distilling the knowledge in a neural network.</strong> Hinton, G., Vinyals, O., & Dean, J. (2015). <a href="http://www.cs.toronto.edu/~hinton/absps/distillation.pdf">[paper]</a></li>

<li><strong>Transferring
knowledge to smaller network with class-distance
loss.</strong> Kim, S. W. & Kim, H. E. (2017). In: ICLRW. <a href="https://openreview.net/pdf?id=ByXrfaGFe">[paper]</a></li>

<li><strong>Improved knowledge distillation via teacher assistant.</strong>Mirzadeh, S. I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., & Ghasemzadeh, H. (2020, April). AAAI. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5963">[paper]</a></li>

<li><strong>Learning efficient object detection models with knowledge distillation.</strong>Chen, G., Choi, W., Yu, X., Han, T., & Chandraker, M. (2017). In: NeurIPS. <a href="https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf">[paper]</a></li>

<li><strong>Conditional teacher-student learning.</strong>Meng, Z., Li, J., Zhao, Y., & Gong, Y. (2019, May). In: ICASSP. <a href="https://arxiv.org/pdf/1904.12399">[paper]</a></li>

<li><strong>Feature fusion for online mutual knowledge distillation.</strong>Kim, J., Hyun, M., Chung, I., & Kwak, N. (2019). In: ICPR. <a href="https://arxiv.org/pdf/1904.09058.pdf">[paper]</a></li>

<li><strong>When does label smoothing help?</strong>MÃ¼ller, R., Kornblith, S., & Hinton, G. E. (2019). In: NeurIPS. <a href="https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf">[paper]</a></li>

</ul>

### Feature-Based Knowledge

<li><strong>WFitnets: Hints for thin deep nets.</strong>Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2014). In: ICLR. <a href="https://arxiv.org/pdf/1412.6550.pdf?source=post_page---------------------------">[paper]</a></li>

<li><strong>Paying more
attention to attention: Improving the performance of
convolutional neural networks via attention transfer.</strong>Zagoruyko, S., & Komodakis, N. (2016).  In: ICLR. <a href="https://arxiv.org/pdf/1612.03928.pdf">[paper]</a></li>

<li><strong>Spatiotemporal distilled dense-connectivity network for video action recognition.</strong>Hao, W., & Zhang, Z. (2019). Pattern Recognition. <a href="http://159.226.21.68/bitstream/173211/23347/1/Spatiotemporal%20distilled%20denseConnectivity%20network%20for%20video%20action%20recognition.pdf">[paper]</a></li>

<li><strong>Learning deep representations with probabilistic knowledge transfer.</strong>Passalis, N. & Tefas, A. (2018).  In: ECCV. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.pdf">[paper]</a></li>

<li><strong>Paraphrasing complex network: Network compression via factor transfer.</strong> Kim, J., Park, S., & Kwak, N. (2018). In: NeurIPS <a href="https://proceedings.neurips.cc/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation via
route constrained optimization. </strong>Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D.,
Yan, J. & Hu, X. (2019). In: ICCV. <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Jin_Knowledge_Distillation_via_Route_Constrained_Optimization_ICCV_2019_paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation with adversarial samples
supporting decision boundary.</strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4263/4141">[paper]</a></li>

<li><strong>Rocket launching: A universal and efficient framework for training well-performing light net.</strong>Zhou, G., Fan, Y., Cui, R., Bian, W., Zhu, X., & Gai, K. (2017).  <a href="https://arxiv.org/pdf/1708.04106">[paper]</a></li>

<li><strong>Differentiable Feature Aggregation Search for Knowledge Distillation.</strong>Guan, Y., Zhao, P., Wang, B., Zhang, Y., Yao, C., Bian, K., & Tang, J. (2020, August). In: ECCV <a href="https://arxiv.org/pdf/2008.00506">[paper]</a></li>

<li><strong> Amalgamating knowledge towards comprehensive classification</strong>Shen, C., Wang, X., Song, J., Sun, L., & Song, M.
(2019). In: AAAI. <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4165/4043">[paper]</a></li>

### Relation-Based Knowledge

<li><strong></strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="">[paper]</a></li>

### - Relationships between Intermediate layers

### - Parameters of teacher model

## Different mode of distillation

## KD algorithms

## KD Applications

<ul>
<li> Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN
</ul>