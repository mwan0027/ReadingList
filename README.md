# About

Reading List of recent published Knowledge-Distillation papers

Reading Priority:
Top: :star2:
Second :star: 

Reading Completion:
:heavy_check_mark:

# Read [Online Version](https://readingpapers.academy/)
<STRONG>Inpage Elevater </STRONG>

| Conderence |    2021  |     2020  |    2019  |   2018   |
| ---------- |-----------------------| -----------------------|-----------------------|-----------------------|
| AAAI       | [AAAI2021](#AAAI-2021) |  [AAAI2020](#AAAI-2020)  |                       |                       |
| ACL        |  NA                   |  [ACL2020](#ACL-2020)   |                       |                       |
| SIGIR      |  NA                   |  [SIGIR2020](#SIGIR-2020) |   |   |
| IJCAI      |  NA                   |  [IJCAI2020](#IJCAI-2020) |   |   |
| ICLR       | [ICLR2021](#ICLR-2021)|  [ICLR2020](#ICLR-2020) |   |   |
| NIPS       |  NA                   |  [NIPS2020](#NIPS-2020) | [NIPS2019](#NIPS-2019)  |   |
| EMNLP      |  NA                   |  [EMNLP2020](#EMNLP-2020) |   |   |
| ECCV       |  TBD                  |  [ECCV2020](#ECCV-2020) |   |   |
| CVPR       | [CVPR2021](#CVPR-2021)|  [CVPR2020](#CVPR-2020) |   |   |
| ICML       | NA |  [ICML2020](#ICML-2020) |   |   |

# How to Run this project
<ul>
    <li>
        <code>git clone https://github.com/mwan0027/ReadingList.git</code>
    </li>
    <li>
        <code>pip install --upgrade pip</code>
    </li>
    <li>
        <code>pip install mkdocs</code>
    </li>
    <li>
        <code>mkdocs serve</code>
    </li>
</ul>

# Full Paper List


## Knowledge Distillation

### [CVPR 2021](http://cvpr2021.thecvf.com/)
 <ul>
    <li>Data-Free Knowledge Distillation For Image Super-Resolution<a href="">[paper]</a>
    <li>There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge<a href="https://arxiv.org/pdf/2103.01353">[paper]</a><a href="https://rl.uni-freiburg.de/">[project]</a> 
    <li>General Instance Distillation for Object Detection<a href="https://arxiv.org/abs/2103.02340">[paper]</a>
    <li>Distilling Object Detectors via Decoupled Features <a href="">[paper_tobe_updated]</a>
    <li>Multiresolution Knowledge Distillation for Anomaly Detectio<a href="https://arxiv.org/abs/2011.11108">[paper]</a>
    <li>[WACV2021]Data-free Knowledge Distillation for Object Detection<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Chawla_Data-Free_Knowledge_Distillation_for_Object_Detection_WACV_2021_paper.pdf">[paper]</a>
</ul>



### [ICLR 2021](https://iclr.cc/)

<ul>
<li>Undistillable: Making A Nasty Teacher That CANNOT teach students <a href="https://openreview.net/pdf?id=0zvfm-nZqQs">[paper]</a> :heavy_check_mark: 
<li>Generalization bounds via distillation<a href="https://openreview.net/pdf?id=EGdFhBzmAwB">[paper]</a>
<li>MixKD: Towards Efficient Distillation of Large-scale Language Models<a href="https://openreview.net/pdf?id=UFGEelJkLu5">[paper]</a> :heavy_check_mark:
<li>Knowledge distillation via softmax regression representation learning<a href="https://openreview.net/pdf?id=ZzwDy_wiWv">[paper]</a>
<li>Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors<a href="https://openreview.net/pdf?id=uKhGRvM8QNH">[paper]</a>
<li>SEED: Self-supervised Distillation For Visual Representation<a href="https://openreview.net/pdf?id=AHm3dbp7D1D">[paper]</a>
<li>Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks <a href="https://openreview.net/pdf?id=9l0K4OM-oXE">[paper]</a>
<li>Distilling Knowledge from Reader to Retriever for Question Answering<a href="https://openreview.net/pdf?id=NTEz-6wysdb">[paper]</a>
<li>Rethinking Soft Labels for Knowledge Distillation: A Biasâ€“Variance Tradeoff Perspective<a href="https://openreview.net/pdf?id=gIHd-5X324">[paper]</a>
<li>Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study<a href="https://openreview.net/pdf?id=PObuuGVrGaZ">[paper]</a>
<li>Knowledge Distillation as Semiparametric Inference<a href="https://openreview.net/pdf?id=m4UCf24r0Y">[paper]</a>
<li>A teacher-student framework to distill future trajectories<a href="https://openreview.net/pdf?id=ECuvULjFQia">[paper]</a>
</ul>

### [AAAI 2021](https://aaai.org/Conferences/AAAI-21/)
<ul>
<li>PSSM-Distil: Protein Secondary Structure Prediction (PSSP) on Low-Quality PSSM by Knowledge Distillation with Contrastive Learning. <a href="">[paper_ToBeUpdate]</a>
<li>Probing Product Description Generation via Posterior Distillation <a href="">[paper_ToBeUpdate]</a>
<li>LRC-BERT: Latent-Representation Contrastive Knowledge Distillation for Natural Language Understanding<a href="https://arxiv.org/pdf/2012.07335">[paper]</a>
<li>Peer Collaborative Learning for Online Knowledge Distillation<a href="https://arxiv.org/pdf/2006.04147">[paper]</a>
<li>Cross-Layer Distillation with Semantic Calibration<a href="https://arxiv.org/pdf/2012.03236">[paper]</a>
<li>Few-Shot Class-Incremental Learning via Relation Knowledge Distillation<a href="">[paper_ToBeUpdate]</a>
<li> Harmonized Dense Knowledge Distillation Training for Multi-Exit Architectures<a href="">[paper_ToBeUpdate]</a>
<li>Universal Trading for Order Execution with Oracle Policy Distillation<a href="">[paper_ToBeUpdate]</a>
<li>Diverse Knowledge Distillation for End-to-End Person Search<a href="https://arxiv.org/pdf/2012.11187">[paper]</a>
<li>Distilling Localization for Self-Supervised Representation Learning<a href="https://arxiv.org/pdf/2004.06638">[paper]</a>
<li>Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation<a href="https://arxiv.org/pdf/2101.08106">[paper]</a>
<li>Multi-View Feature Representation for Dialogue Generation with Bidirectional Distillation<a href="https://jklj077.github.io/publication/aaai2021-multiview/">[paper_ToBeUpdate]</a>
<li> Data-Free Knowledge Distillation with Soft Targeted Transfer Set Synthesis<a href="">[paper_ToBeUpdate]</a>
<li> ALP-KD: Attention-Based Layer Projection for Knowledge Distillation<a href="https://arxiv.org/pdf/2012.14022">[paper]</a>
<li>Progressive Network Grafting for Few-Shot Knowledge Distillation<a href="https://arxiv.org/pdf/2012.04915">[paper]</a>
<li> Show, Attend and Distill: Knowledge Distillation via Attention-Based Feature Matching<a href="https://arxiv.org/pdf/2102.02973">[paper]</a>
<li>Anytime Inference with Distilled Hierarchical Neural Ensembles<a href="">[paper_ToBeUpdate]</a>
<li>Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks<a href="https://arxiv.org/pdf/2009.14502">[paper]</a>
<li>Reinforced Multi-Teacher Selection for Knowledge Distillation<a href="https://arxiv.org/pdf/2012.06048">[paper]</a>
<li> Deep Contextual Clinical Prediction with Reverse Distillation<a href="https://arxiv.org/pdf/2007.05611">[paper]</a>

</ul>

### [AAAI 2020](https://aaai.org/Conferences/AAAI-20/)
<ul>
<li>Uncertainty-aware Multi-shot Knowledge Distillation for Image-based Object Re-identification. <a href="https://www.msra.cn/wp-content/uploads/2020/01/Uncertainty-aware-Multi-shot-Knowledge-Distillation-for-Image-based-Object-Re-identification.pdf">[paper]</a>:heavy_check_mark:
<li>Towards Cross-modality Medical Image Segmentation with Online Mutual Knowledge Distillation. <a href="https://www.researchgate.net/profile/Lequan_Yu/publication/341886190_Towards_Cross-Modality_Medical_Image_Segmentation_with_Online_Mutual_Knowledge_Distillation/links/5edcb14292851c9c5e8b0b51/Towards-Cross-Modality-Medical-Image-Segmentation-with-Online-Mutual-Knowledge-Distillation.pdf">[paper]</a>:heavy_check_mark:
<li>Ultrafast Video Attention Prediction with Coupled Knowledge Distillation. <a href="https://arxiv.org/pdf/1904.04449">[paper]</a>:heavy_check_mark:
<li>Online Knowledge Distillation with Diverse Peers. <a href="https://www.researchgate.net/profile/Jian-Ping_Mei/publication/337413771_Online_Knowledge_Distillation_with_Diverse_Peers/links/5dd60feb458515cd48aff978/Online-Knowledge-Distillation-with-Diverse-Peers.pdf">[paper]</a>
<li>Towards Oracle Knowledge Distillation with Neural Architecture Search. <a href="https://www.researchgate.net/profile/Lequan_Yu/publication/341886190_Towards_Cross-Modality_Medical_Image_Segmentation_with_Online_Mutual_Knowledge_Distillation/links/5edcb14292851c9c5e8b0b51/Towards-Cross-Modality-Medical-Image-Segmentation-with-Online-Mutual-Knowledge-Distillation.pdf">[paper]</a>:heavy_check_mark:
<li>Knowledge Distillation from Internal Representations. <a href="https://assets.amazon.science/7a/a6/7e6c8be54bd4a9674ba3826988e3/knowledge-distillation-from-internal-rpresentations.pdf">[paper]</a>
<li>Improved Knowledge Distillation via Teacher Assistant. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/5963/5819">[paper]</a> <a href="https://imirzadeh.me/dl/TAKD_poster.pdf">[poster]</a>:heavy_check_mark:
<li>Distilling portable Generative Adversarial Networks for Image Translation. <a href="https://arxiv.org/pdf/2003.03519.pdf">[paper]</a>
<li>Look One and More: Distilling Hybrid Order Relational Knowledge for Cross-Resolution Image Recognition. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6715/6569">[paper]</a>
<li>Learning end-to-end scene flow by distilling single tasks knowledge. <a href="https://www.researchgate.net/profile/Stefano_Mattoccia/publication/337484940_Learning_End-To-End_Scene_Flow_by_Distilling_Single_Tasks_Knowledge/links/5ddb79eda6fdccdb4462c695/Learning-End-To-End-Scene-Flow-by-Distilling-Single-Tasks-Knowledge.pdf">[paper]</a>
<li>Few shot network compression via cross distillation. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5718/5574">[paper]</a>
<li>Adversarially Robust Distillation. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/5816/5672">[paper]</a>
<li>Multi-source Distilling Domain Adaptation. [TBD]<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6997/6851">[paper]</a>
<li>Long Short-Term Sample Distillation. <a href="https://arxiv.org/pdf/2003.00739.pdf">[paper]</a>
<li> Scalable Attentive Sentence-Pair Modeling via Distilled Sentence Embedding. <a href="https://arxiv.org/pdf/1908.05161.pdf">[paper]</a>
<li>Distilling Knowledge from Well-informed Soft Labels for Neural Relation Extraction. <a href="https://arxiv.org/pdf/1908.05161">[paper]</a>
<li>Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers. <a href="https://www.researchgate.net/profile/Xinchao_Wang/publication/337560210_Hearing_Lips_Improving_Lip_Reading_by_Distilling_Speech_Recognizers/links/5e2fc35992851c9af729e6fe/Hearing-Lips-Improving-Lip-Reading-by-Distilling-Speech-Recognizers.pdf">[paper]</a>
<li>Filtration and Distillation: Enhancing Region Attention for Fine-Grained Visual Categorization. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/6822/6676">[paper]</a>
</ul>


### [ACL 2020](https://www.aclweb.org/anthology/events/acl-2020/#2020acl-main)
<ul>
<li>Distilling Knowledge Learned in BERT for Text Generation<a href="https://www.aclweb.org/anthology/2020.acl-main.705.pdf">[paper]</a><a href="https://github.com/ChenRocks/Distill-BERT-Textgen">[code]</a>:heavy_check_mark:
<li>Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT<a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.10.pdf">[paper]</a> <a href="https://github.com/castorini/hedwig">[code]</a> :heavy_check_mark:
<li>End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.8.pdf">[paper]</a>:heavy_check_mark:
<li>Distilling Neural Networks for Greener and Faster Dependency Parsing<a href="https://www.aclweb.org/anthology/2020.iwpt-1.2.pdf">[paper]</a> <a href="https://github.com/yzhangcs/parser">[Speed Measure code]</a>:heavy_check_mark:
<li>XtremeDistil: Multi-stage Distillation for Massive Multilingual Models<a href="https://www.aclweb.org/anthology/2020.acl-main.202.pdf">[paper]</a><a href="https://github.com/MSR-LIT/XtremeDistil/">[code]</a> :heavy_check_mark:
<li>Structure-Level Knowledge Distillation For
Multilingual Sequence Labeling<a href="https://www.aclweb.org/anthology/2020.acl-main.304.pdf">[paper]</a><a href="https://github.com/Alibaba-NLP/MultilangStructureKD">[code]</a> 
<li>Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation<a href="https://www.aclweb.org/anthology/2020.acl-main.324.pdf">[paper]</a>
<li>FastBERT: a Self-distilling BERT with Adaptive Inference Time<a href="https://www.aclweb.org/anthology/2020.acl-main.537.pdf">[paper]</a><a href="https://github.com/autoliuweijie/FastBERT">[code]</a>:heavy_check_mark:
<li>TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural
Language Processing<a href="https://www.aclweb.org/anthology/2020.acl-demos.2.pdf">[paper]</a><a href="https://github.com/airaria/TextBrewer">[code]</a>:heavy_check_mark:
<li>Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation<a href="https://www.aclweb.org/anthology/2020.ngt-1.12.pdf">[paper]</a><a href="https://github.com/mitchellgordon95/kd-adapt">[code]</a>:heavy_check_mark:

</ul>

### [CVPR 2020](https://openaccess.thecvf.com/CVPR2020_search)
 
 <ul>
 <li>Spatio-Temporal Graph for Video Captioning With Knowledge Distillation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>
 <li>Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yin_Dreaming_to_Distill_CVPR_2020_supplemental.pdf">[supp]</a> :heavy_check_mark:
 <li>Search to Distill: Pearls Are Everywhere but Not the Eyes <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Search_to_Distill_Pearls_Are_Everywhere_but_Not_the_Eyes_CVPR_2020_paper.pdf">[paper]</a>
 <li>Explaining Knowledge Distillation by Quantifying the Knowledge<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf">[paper]</a>
 <li>Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Networks_Are_More_Productive_Teachers_Than_Human_Raters_Active_CVPR_2020_paper.pdf">[paper]</a> <a href="https://www.youtube.com/watch?v=yBO8olcWHvE">[video]</a> 
 <li>Regularizing Class-Wise Predictions via Self-Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yun_Regularizing_Class-Wise_Predictions_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Creating_Something_From_Nothing_Unsupervised_Knowledge_Distillation_for_Cross-Modal_Hashing_CVPR_2020_paper.pdf">[paper]</a><a href="https://www.youtube.com/watch?v=Io1uloVOEJk">[video]</a> 
 <li>Heterogeneous Knowledge Distillation Using Information Flow Modeling<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Passalis_Heterogeneous_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Revisiting Knowledge Distillation via Label Smoothing Regularization<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yuan_Revisiting_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> :heavy_check_mark:
 <li>Collaborative Distillation for Ultra-Resolution Universal Style Transfer<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.pdf">[paper]</a>
 <li>Few Sample Knowledge Distillation for Efficient Network Compression<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Few_Sample_Knowledge_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Online Knowledge Distillation via Collaborative Learning<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf">[paper]</a>
 <li>Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Block-Wisely_Supervised_Neural_Architecture_Search_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Block-Wisely_Supervised_Neural_CVPR_2020_supplemental.pdf">[supp]</a> <a href="https://www.youtube.com/watch?v=DCBYhdCPpoA">[video]</a> 
 </ul>


<!-- #SIGIR -->
### [SIGIR 2020](http://static.ijcai.org/2020-accepted_papers.html)
<ul>
<li>Differentially Private Knowledge Distillation for Mobile Analytics [short paper] <a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401259"> [page]</a>
<li>Distilling Knowledge for fast retrieval-based chat-bots [short paper]<a href="https://arxiv.org/pdf/2004.11045.pdf">[paper]</a>
<li>A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data <a href="http://csse.szu.edu.cn/staff/panwk/publications/Conference-SIGIR-20-KDCRec.pdf">[paper]</a>
<a href="https://github.com/dgliu/SIGIR20_KDCRec.">[code]</a>
</ul>


<!-- #IJCAI -->
### [IJCAI 2020](http://static.ijcai.org/2020-accepted_papers.html)
<ul>
<li>P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection <a href="https://www.ijcai.org/Proceedings/2020/0448.pdf">[paper]</a>
</ul>

<!-- #ECCV -->
### [ECCV 2020](https://eccv2020.eu/)
<ul>
<li>Circumventing Outliers of AutoAugment with Knowledge Distillation<a href="">[paper]</a>
<li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480613.pdf">[paper]</a>
<li>Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500239.pdf">[paper]</a>
<li>Knowledge Distillation Meets Self-Supervision<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562-supp.pdf">[supp]</a>
<li>Local Correlation Consistency for Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018-supp.zip">[supp]</a>
<li>Robust Re-Identification by Multiple Views Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550103.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550103-supp.pdf">[supp]</a>
<li>AMLN: Adversarial-based Mutual Learning Network for Online Knowledge Distilla<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570154.pdf">[paper]</a>
<li>Differentiable Feature Aggregation Search for Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620460.pdf">[paper]</a>
:heavy_check_mark:<li>Online Ensemble Model Compression using Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640018.pdf">[paper]</a>
<li>Exclusivity-Consistency Regularized Knowledge Distillation for Face Recognition<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690324.pdf">[paper]</a>
<li>Feature Normalized Knowledge Distillation for Image Classification<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700664.pdf">[paper]</a>
<li>Weight Decay Scheduling and Knowledge Distillation for Active Learning<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710426.pdf">[paper]</a>
<li>Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710647.pdf">[paper]</a><a href="hhttps://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710647-supp.pdf">[supp]</a>
<li>Interpretable Foreground Object Search As Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730188.pdf">[paper]</a>
<li>Improving Knowledge Distillation via Category Structure<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730205.pdf">[paper]</a>
<li>Circumventing Outliers of AutoAugment with Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480613.pdf">[paper]</a>
</ul>


### [ICLR 2020](https://iclr.cc/virtual_2020/papers.html?filter=keywords&search=knowledge+distillation)

<ul>
<li>Neural Epitome Search for Architecture-Agnostic Network Compression <a href="https://openreview.net/pdf?id=HyxjOyrKvr">[paper]</a>
<li>Contrastive Representation Distillation<a href="http://www.openreview.net/pdf?id=SkgpBJrtvS">[paper]</a><a href="https://github.com/HobbitLong/RepDistiller">[code]</a>
<li>Ensemble Distribution Distillation<a href="http://www.openreview.net/pdf?id=BygSP6Vtvr">[paper]</a>
<li>Understanding Knowledge Distillation in Non-autoregressive Machine Translation<a href="http://www.openreview.net/pdf?id=BygFVAEKDH">[paper]</a>
<li>Neural Epitome Search for Architecture-Agnostic Network Compression<a href="http://www.openreview.net/pdf?id=HyxjOyrKvr">[paper]</a>
</ul>

### [EMNLP 2020](https://2020.emnlp.org/papers/main)
<ul>
<li>FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.165.pdf">[paper]</a>:heavy_check_mark:
<li>Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. <a href="https://arxiv.org/pdf/2004.09813">[paper]</a>
<li>Adversarial Self-Supervised Data-Free Distillation for Text Classification<a href="https://www.aclweb.org/anthology/2020.emnlp-main.499.pdf">[paper]</a> :heavy_check_mark:
<li>Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data<a href="https://www.aclweb.org/anthology/2020.emnlp-main.277.pdf">[paper]</a>
<li>Lifelong Language Knowledge Distillation.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.233.pdf">[paper]</a>
<li>Autoregressive Knowledge Distillation through Imitation Learning.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.494.pdf">[paper]</a>
<li>TernaryBERT: Distillation-aware Ultra-low Bit BERT.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.37.pdf">[paper]</a>
<li>Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast---Choose Three. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.450.pdf">[paper]</a>
<li>Improving Neural Topic Models using Knowledge Distillation. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.137.pdf">[paper]</a>
<li>Contrastive Distillation on Intermediate Representations for Language Model Compression.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.36.pdf">[paper]</a>
<li>Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers. [short paper] <a href="https://www.aclweb.org/anthology/2020.emnlp-main.74.pdf">[paper]</a>
</ul>

### [NIPS 2020](https://papers.nips.cc/paper/2020)
<ul>
<li>Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher <a href="https://papers.nips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Supplemental.zip">[supp]</a>
<li>Task-Oriented Feature Distillation <a href="https://papers.nips.cc/paper/2020/file/a96b65a721e561e1e3de768ac819ffbb-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/a96b65a721e561e1e3de768ac819ffbb-Supplemental.zip">[supp]</a>
<li>Kernel Based Progressive Distillation for Adder Neural Networks <a href="https://papers.nips.cc/paper/2020/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/912d2b1c7b2826caf99687388d2e8f7c-Supplemental.pdf">[supp]</a>
<li>Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection<a href="https://papers.nips.cc/paper/2020/file/c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/c3535febaff29fcb7c0d20cbe94391c7-Supplemental.pdf">[supp]</a>
<li>Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search<a href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Supplemental.pdf">[supp]</a>
<li>Self-Distillation Amplifies Regularization in Hilbert Space<a href="https://papers.nips.cc/paper/2020/file/2288f691b58edecadcc9a8691762b4fd-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/2288f691b58edecadcc9a8691762b4fd-Supplemental.zip">[supp]</a>
<li>MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers<a href="https://papers.nips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Supplemental.pdf">[supp]</a>
<li>Self-Distillation as Instance-Specific Label Smoothing<a href="https://papers.nips.cc/paper/2020/file/1731592aca5fb4d789c4119c65c10b4b-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/1731592aca5fb4d789c4119c65c10b4b-Supplemental.pdf">[supp]</a>
<li>Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space <a href="https://papers.nips.cc/paper/2020/file/91c77393975889bd08f301c9e13a44b7-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/91c77393975889bd08f301c9e13a44b7-Supplemental.pdf">[supp]</a>
<li>Distributed Distillation for On-Device Learning<a href="https://papers.nips.cc/paper/2020/file/fef6f971605336724b5e6c0c12dc2534-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/fef6f971605336724b5e6c0c12dc2534-Supplemental.pdf">[supp]</a>
<li>Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts<a href="https://papers.nips.cc/paper/2020/file/657b96f0592803e25a4f07166fff289a-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/657b96f0592803e25a4f07166fff289a-Supplemental.zip">[supp]</a>
<li>Ensemble Distillation for Robust Model Fusion in Federated Learning<a href="https://papers.nips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Supplemental.pdf">[supp]</a>:heavy_check_mark:
<li>Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation<a href="https://papers.nips.cc/paper/2020/file/62d75fb2e3075506e8837d8f55021ab1-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/62d75fb2e3075506e8837d8f55021ab1-Supplemental.zip">[supp]</a>
</ul>



### [ICML 2020](https://icml.cc/virtual/2020)

<ul>
<li>AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks <a href="https://arxiv.org/pdf/2006.08198">[paper]</a><a href="https://
github.com/TAMU-VITA/AGD">[code]</a>
<li>Feature-map-level Online Adversarial Knowledge Distillation<a href="https://arxiv.org/pdf/2002.01775">[paper]</a>
<li>Dual-Path Distillation: A Unified Framework to Improve Black-Box Attacks<a href="http://proceedings.mlr.press/v119/zhang20o/zhang20o.pdf">[paper]</a>

</ul>



### [NIPS 2019](https://papers.nips.cc/paper/2019)

<ul>
<li>FedMD: Heterogenous Federated Learning via Model Distillation<a href="https://arxiv.org/pdf/1910.03581.pdf">[paper]</a><a href="https://github.com/diogenes0319/FedMD_clean">[code]</a>:heavy_check_mark:

</ul>


## Federated Learning

### FL open question & useful framework
<ul>
<li>:star2:Advances and Open Problems in Federated Learning<a href="https://arxiv.org/pdf/1912.04977">[paper]</a>

<li>:star:Split learning for health: Distributed deep learning without sharing raw patient data <a href="https://arxiv.org/pdf/1812.00564.pdf">[paper]</a>

<li>:star:Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge <a href="https://proceedings.neurips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf">[paper]</a>

<li>Split learning for collaborative deep learning in healthcare<a href="https://arxiv.org/pdf/1912.12115">[paper]</a>

<li>Splitfed: When federated learning meets split learning<a href="https://arxiv.org/pdf/2004.12088">[paper]</a>

<li>Detailed comparison of communication efficiency of split learning and federated learning <a href="https://arxiv.org/pdf/1909.09145">[paper]</a>

</ul>

### FD KD NLP
<ul>
<li>:star:Improving question answering by commonsense-based pre-training
<a href="https://arxiv.org/pdf/1809.03568">[paper]</a>
<li>:star:Collaborative fairness in federated learning<a href="https://arxiv.org/pdf/2008.12161">[paper]</a>
<li>:star:CovidNLP: a web application for distilling systemic implications of COVID-19 pandemic with Natural Language processing<a href="https://www.medrxiv.org/content/medrxiv/early/2020/04/29/2020.04.25.20079129.full.pdf">[paper]</a>
<li>:star:Federated learning for healthcare informatics <a href="https://link.springer.com/article/10.1007/s41666-020-00082-4">[paper]</a>
<li>:star:Ensemble distillation for robust model fusion in federated learning<a href="https://papers.nips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Supplemental.pdf">[paper]</a>:heavy_check_mark:
<li>:star:Communication-Efficient Federated Distillation
<a href="https://arxiv.org/pdf/2012.00632">[paper]</a>
<li>:star:Sentiment detection with FedMD: Federated Learning via Model Distillation<a href="https://www.researchgate.net/profile/Galina_Momcheva/publication/347172951_Sentiment_detection_with_FedMD_Federated_Learning_via_Model_Distillation/links/5fd8731c299bf140880f7e4a/Sentiment-detection-with-FedMD-Federated-Learning-via-Model-Distillation.pdf">[paper]</a>
<li>:star:FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction<a href="https://www.aclweb.org/anthology/2020.emnlp-main.165.pdf">[paper]</a>
</ul>


### FL on [ICLR 2021](https://iclr.cc/)
<ul>
<li>Federated Learning Based on Dynamic Regularization<a href="https://openreview.net/pdf?id=B7v4QMR6Z9w">[paper]</a>
<li>Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms<a href="https://openreview.net/pdf?id=GFsU8a0sGB">[paper]</a>
<li>Adaptive Federated Optimization<a href="https://openreview.net/pdf?id=LkFG3lB13U5">[paper]</a>
<li>Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning<a href="https://openreview.net/pdf?id=jDdzh5ul-d">[paper]</a>
<li>Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning<a href="https://openreview.net/pdf?id=ce6CFXBh30h">[paper]</a>
<li>FedBN: Federated Learning on Non-IID Features via Local Batch Normalization <a href="https://openreview.net/pdf?id=6YEQUn0QICG">[paper]</a>
<li>FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning<a href="https://openreview.net/pdf?id=dgtpE6gKjHn">[paper]</a>
<li>FedMix: Approximation of Mixup under Mean Augmented Federated Learning<a href="https://openreview.net/pdf?id=Ogga20D2HO-">[paper]</a>
<li>HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients <a href="https://openreview.net/pdf?id=TNkPBBYFkXg">[paper]</a>
<li>Personalized Federated Learning with First Order Model Optimization<a href="https://openreview.net/pdf?id=ehJqJQk9cw">[paper]</a>
</ul>

### FL on [ICML 2020](https://icml.cc/virtual/2020/)
<ul>
<li>Federated Learning with Only Positive Labels<a href="http://proceedings.mlr.press/v119/yu20f/yu20f.pdf">[paper]</a>
<li>FetchSGD: Communication-Efficient Federated Learning with Sketching<a href="http://proceedings.mlr.press/v119/rothchild20a/rothchild20a.pdf">[paper]</a>
<li>SCAFFOLD: Stochastic Controlled Averaging for Federated Learning<a href="http://proceedings.mlr.press/v119/karimireddy20a/karimireddy20a.pdf">[paper]</a>
<li>From Local SGD to Local Fixed-Point Methods for Federated Learning<a href="http://proceedings.mlr.press/v119/malinovskiy20a/malinovskiy20a.pdf">[paper]</a>
<li>FedBoost: A Communication-Efficient Algorithm for Federated Learning<a href="http://proceedings.mlr.press/v119/hamer20a/hamer20a.pdf">[paper]</a>

</ul>

### FD ML crossfield
<ul>
<li>Fedmd: Heterogenous federated learning via model distillation<a href="https://arxiv.org/pdf/1910.03581">[paper]</a>:heavy_check_mark:
<li>Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data<a href="https://arxiv.org/pdf/1811.11479">[paper]</a>
:heavy_check_mark:<li>Ensemble distillation for robust model fusion in federated learning<a href="https://papers.nips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Supplemental.pdf">[paper]</a>
<li>Wireless federated distillation for distributed edge learning with heterogeneous data<a href="https://arxiv.org/pdf/1907.02745">[paper]</a>
<li>:star2:Federated Knowledge Distillation (Not published yet)<a href="https://arxiv.org/pdf/2011.02367.pdf">[paper]</a>
<li>Federated Learning with Heterogeneous Labels and Models for Mobile Activity Monitoring<a href="https://arxiv.org/pdf/2012.02539">[paper]</a>
<li>Hierarchical federated learning across heterogeneous cellular networks<a href="https://arxiv.org/pdf/1909.02362">[paper]</a>
<li>Federated Mutual Learning<a href="https://arxiv.org/pdf/2006.16765">[paper]</a>
<li>Model pruning enables efficient federated learning on edge devices<a href="https://arxiv.org/pdf/1909.12326">[paper]</a>
</ul>

### IEEE
<strong>[IEEE ICKG 2020](http://ickg2020.bigke.org/)</strong>
<ul>
<li>Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9194566">[paper]</a>
</ul>

 <strong>[IEEE Signal Processing Magazine 2020](http://ickg2020.bigke.org/)</strong>
<ul>
<li>Federated Learning: Challenges, Methods, and Future Directions<a href="https://ieeexplore.ieee.org/abstract/document/9084352">[paper]</a>
</ul>

 <strong>[IEEE Transactions on Parallel and Distributed Systems 2020](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=71)</strong>
<ul>
<li>Mutual Information Driven Federated Learning<a href="https://ieeexplore.ieee.org/abstract/document/9272656">[paper]</a>
</ul>


### Surveys
<strong>[ACM TIST 2019](https://dl.acm.org/journal/tist)</strong>
<ul>
<li>Federated Machine Learning: Concept and Applications <a href="http://sites.nlsde.buaa.edu.cn/~yxtong/tist_fl.pdf">[paper]</a>

<li>A survey on security and privacy of federated learning <a href="https://www.sciencedirect.com/science/article/pii/S0167739X20329848">[paper]</a>
</ul>

<ul>
<li>Generalizing from a few examples: A survey on few-shot learning (CSUR 2020) <a href="https://arxiv.org/pdf/1904.05046">[paper]</a>

<li> [IEEE Communications Surveys & Tutorials 2019] Federated learning in mobile edge networks: A comprehensive survey <a href="https://arxiv.org/pdf/1909.11875">[paper]</a>

<li> [IEEE, 2020] Federated learning: A survey on enabling technologies, protocols, and applications <a href="https://ieeexplore.ieee.org/iel7/6287639/8948470/09153560.pdf">[paper]</a>

<li>From Federated Learning to Federated Neural Architecture Search: A Survey <a href="https://arxiv.org/pdf/2009.05868">[paper]</a>

</ul>

### Applications:
<ul>
<li>A review of applications in federated learning<a href="https://www.sciencedirect.com/science/article/pii/S0360835220305532">[paper]</a>
<li>Federated learning in medicine: facilitating multiâ€‘institutional collaborations without sharing patient data<a href="https://www.nature.com/articles/s41598-020-69250-1">[article]</a>
</ul>


## Topic Models
<strong>[EMNLP 2020](https://2020.emnlp.org/papers/main)</strong>
<ul>
<li>Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models<a href="https://www.aclweb.org/anthology/2020.emnlp-main.234/">[paper]</a><a href="https://github.com/aterenin/Parallel-HDP-Experiments/">[code]</a>:heavy_check_mark:
<li>Short Text Topic Modeling with Topic Distribution Quantization and Negative Sampling Decoder<a href="https://www.aclweb.org/anthology/2020.emnlp-main.138/">[paper]</a><a href="https://github.com/BobXWu/NQTM">[code]</a>:heavy_check_mark:
<li>Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding<a href="https://arxiv.org/pdf/2010.06705">[paper]</a>
<li>Neural Topic Modeling with Cycle-Consistent Adversarial Training<a href="https://arxiv.org/pdf/2009.13971">[paper]</a>
<li>Improving Neural Topic Models using Knowledge Distillation<a href="https://arxiv.org/pdf/2010.02377">[paper]</a>
<li>Friendly Topic Assistant for Transformer Based Abstractive Summarization<a href="https://www.aclweb.org/anthology/2020.emnlp-main.35.pdf">[paper]</a>
<li>Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference<a href="https://arxiv.org/pdf/2009.09364">[paper]</a>


</ul>


<hr/>
<hr/>



# To-Do: Knowledge Distillation Survey Paper List
List followed the work from Gou. et. al. (2020)[Knowledge Distillation Survery](https://arxiv.org/pdf/2006.05525.pdf)
## Different form of Knowledge

### Response-Based Knowledge
<ul>
<li> <strong>Do deep nets really need to be deep?</strong> Ba, J., & Caruana, R. (2014). NIPS <a href="https://papers.nips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf">[paper]</a></li> 


<li> <strong>Distilling the knowledge in a neural network.</strong> Hinton, G., Vinyals, O., & Dean, J. (2015). <a href="http://www.cs.toronto.edu/~hinton/absps/distillation.pdf">[paper]</a></li>

<li><strong>Transferring
knowledge to smaller network with class-distance
loss.</strong> Kim, S. W. & Kim, H. E. (2017). In: ICLRW. <a href="https://openreview.net/pdf?id=ByXrfaGFe">[paper]</a></li>

<li><strong>Improved knowledge distillation via teacher assistant.</strong>Mirzadeh, S. I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., & Ghasemzadeh, H. (2020, April). AAAI. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5963">[paper]</a></li>

<li><strong>Learning efficient object detection models with knowledge distillation.</strong>Chen, G., Choi, W., Yu, X., Han, T., & Chandraker, M. (2017). In: NeurIPS. <a href="https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf">[paper]</a></li>

<li><strong>Conditional teacher-student learning.</strong>Meng, Z., Li, J., Zhao, Y., & Gong, Y. (2019, May). In: ICASSP. <a href="https://arxiv.org/pdf/1904.12399">[paper]</a></li>

<li><strong>Feature fusion for online mutual knowledge distillation.</strong>Kim, J., Hyun, M., Chung, I., & Kwak, N. (2019). In: ICPR. <a href="https://arxiv.org/pdf/1904.09058.pdf">[paper]</a></li>

<li><strong>When does label smoothing help?</strong>MÃ¼ller, R., Kornblith, S., & Hinton, G. E. (2019). In: NeurIPS. <a href="https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf">[paper]</a></li>

</ul>

### Feature-Based Knowledge

<li><strong>WFitnets: Hints for thin deep nets.</strong>Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2014). In: ICLR. <a href="https://arxiv.org/pdf/1412.6550.pdf?source=post_page---------------------------">[paper]</a></li>

<li><strong>Paying more
attention to attention: Improving the performance of
convolutional neural networks via attention transfer.</strong>Zagoruyko, S., & Komodakis, N. (2016).  In: ICLR. <a href="https://arxiv.org/pdf/1612.03928.pdf">[paper]</a></li>

<li><strong>Spatiotemporal distilled dense-connectivity network for video action recognition.</strong>Hao, W., & Zhang, Z. (2019). Pattern Recognition. <a href="http://159.226.21.68/bitstream/173211/23347/1/Spatiotemporal%20distilled%20denseConnectivity%20network%20for%20video%20action%20recognition.pdf">[paper]</a></li>

<li><strong>Learning deep representations with probabilistic knowledge transfer.</strong>Passalis, N. & Tefas, A. (2018).  In: ECCV. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.pdf">[paper]</a></li>

<li><strong>Paraphrasing complex network: Network compression via factor transfer.</strong> Kim, J., Park, S., & Kwak, N. (2018). In: NeurIPS <a href="https://proceedings.neurips.cc/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation via
route constrained optimization. </strong>Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D.,
Yan, J. & Hu, X. (2019). In: ICCV. <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Jin_Knowledge_Distillation_via_Route_Constrained_Optimization_ICCV_2019_paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation with adversarial samples
supporting decision boundary.</strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4263/4141">[paper]</a></li>

<li><strong>Rocket launching: A universal and efficient framework for training well-performing light net.</strong>Zhou, G., Fan, Y., Cui, R., Bian, W., Zhu, X., & Gai, K. (2017).  <a href="https://arxiv.org/pdf/1708.04106">[paper]</a></li>

<li><strong>Differentiable Feature Aggregation Search for Knowledge Distillation.</strong>Guan, Y., Zhao, P., Wang, B., Zhang, Y., Yao, C., Bian, K., & Tang, J. (2020, August). In: ECCV <a href="https://arxiv.org/pdf/2008.00506">[paper]</a></li>

<li><strong> Amalgamating knowledge towards comprehensive classification</strong>Shen, C., Wang, X., Song, J., Sun, L., & Song, M.
(2019). In: AAAI. <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4165/4043">[paper]</a></li>

### Relation-Based Knowledge

<li><strong></strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="">[paper]</a></li>

### - Relationships between Intermediate layers

### - Parameters of teacher model

## Different mode of distillation

## KD algorithms

## KD Applications

<ul>
<li> Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN
</ul>

