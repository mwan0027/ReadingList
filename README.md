# About

Reading List of recent published Knowledge-Distillation papers
# Read [Online Version](https://readingpapers.academy/)

# How to Run this project
<ul>
    <li>
        <code>git clone https://github.com/mwan0027/ReadingList.git</code>
    </li>
    <li>
        <code>pip install --upgrade pip</code>
    </li>
    <li>
        <code>pip install mkdocs</code>
    </li>
    <li>
        <code>mkdocs serve</code>
    </li>
</ul>

# Full Paper List


## Knowledge Distillation

<strong>[AAAI 2020](https://aaai.org/Conferences/AAAI-20/)</strong>
<ul>
<li>Uncertainty-aware Multi-shot Knowledge Distillation for Image-based Object Re-identification. <a href="https://www.msra.cn/wp-content/uploads/2020/01/Uncertainty-aware-Multi-shot-Knowledge-Distillation-for-Image-based-Object-Re-identification.pdf">[paper]</a>
<li>Towards Cross-modality Medical Image Segmentation with Online Mutual Knowledge Distillation. <a href="https://www.researchgate.net/profile/Lequan_Yu/publication/341886190_Towards_Cross-Modality_Medical_Image_Segmentation_with_Online_Mutual_Knowledge_Distillation/links/5edcb14292851c9c5e8b0b51/Towards-Cross-Modality-Medical-Image-Segmentation-with-Online-Mutual-Knowledge-Distillation.pdf">[paper]</a>
<li>Ultrafast Video Attention Prediction with Coupled Knowledge Distillation. <a href="https://arxiv.org/pdf/1904.04449">[paper]</a>
<li>Online Knowledge Distillation with Diverse Peers. <a href="https://www.researchgate.net/profile/Jian-Ping_Mei/publication/337413771_Online_Knowledge_Distillation_with_Diverse_Peers/links/5dd60feb458515cd48aff978/Online-Knowledge-Distillation-with-Diverse-Peers.pdf">[paper]</a>
<li>Towards Oracle Knowledge Distillation with Neural Architecture Search. <a href="https://www.researchgate.net/profile/Lequan_Yu/publication/341886190_Towards_Cross-Modality_Medical_Image_Segmentation_with_Online_Mutual_Knowledge_Distillation/links/5edcb14292851c9c5e8b0b51/Towards-Cross-Modality-Medical-Image-Segmentation-with-Online-Mutual-Knowledge-Distillation.pdf">[paper]</a>
<li>Knowledge Distillation from Internal Representations. <a href="https://assets.amazon.science/7a/a6/7e6c8be54bd4a9674ba3826988e3/knowledge-distillation-from-internal-rpresentations.pdf">[paper]</a>
<li>Improved Knowledge Distillation via Teacher Assistant. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/5963/5819">[paper]</a> <a href="https://imirzadeh.me/dl/TAKD_poster.pdf">[poster]</a>
<li>Distilling portable Generative Adversarial Networks for Image Translation. <a href="https://arxiv.org/pdf/2003.03519.pdf">[paper]</a>
<li>Look One and More: Distilling Hybrid Order Relational Knowledge for Cross-Resolution Image Recognition. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6715/6569">[paper]</a>
<li>Learning end-to-end scene flow by distilling single tasks knowledge. <a href="https://www.researchgate.net/profile/Stefano_Mattoccia/publication/337484940_Learning_End-To-End_Scene_Flow_by_Distilling_Single_Tasks_Knowledge/links/5ddb79eda6fdccdb4462c695/Learning-End-To-End-Scene-Flow-by-Distilling-Single-Tasks-Knowledge.pdf">[paper]</a>
<li>Few shot network compression via cross distillation. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5718/5574">[paper]</a>
<li>Adversarially Robust Distillation. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/5816/5672">[paper]</a>
<li>Multi-source Distilling Domain Adaptation. [TBD]<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6997/6851">[paper]</a>
<li>Long Short-Term Sample Distillation. <a href="https://arxiv.org/pdf/2003.00739.pdf">[paper]</a>
<li> Scalable Attentive Sentence-Pair Modeling via Distilled Sentence Embedding. <a href="https://arxiv.org/pdf/1908.05161.pdf">[paper]</a>
<li>Distilling Knowledge from Well-informed Soft Labels for Neural Relation Extraction. <a href="https://arxiv.org/pdf/1908.05161">[paper]</a>
<li>Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers. <a href="https://www.researchgate.net/profile/Xinchao_Wang/publication/337560210_Hearing_Lips_Improving_Lip_Reading_by_Distilling_Speech_Recognizers/links/5e2fc35992851c9af729e6fe/Hearing-Lips-Improving-Lip-Reading-by-Distilling-Speech-Recognizers.pdf">[paper]</a>
<li>Filtration and Distillation: Enhancing Region Attention for Fine-Grained Visual Categorization. <a href="https://ojs.aaai.org/index.php/AAAI/article/download/6822/6676">[paper]</a>
</ul>


<strong>[ACL 2020](https://www.aclweb.org/anthology/events/acl-2020/#2020acl-main)</strong>
<ul>
<li>Distilling Knowledge Learned in BERT for Text Generation<a href="https://www.aclweb.org/anthology/2020.acl-main.705.pdf">[paper]</a><a href="https://github.com/ChenRocks/Distill-BERT-Textgen">[code]</a>
<li>Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT<a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.10.pdf">[paper]</a> <a href="https://github.com/castorini/hedwig">[code]</a> 
<li>End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.8.pdf">[paper]</a>
<li>Distilling Neural Networks for Greener and Faster Dependency Parsing<a href="https://www.aclweb.org/anthology/2020.iwpt-1.2.pdf">[paper]</a> <a href="https://github.com/yzhangcs/parser">[Speed Measure code]</a>
<li>XtremeDistil: Multi-stage Distillation for Massive Multilingual Models<a href="https://www.aclweb.org/anthology/2020.acl-main.202.pdf">[paper]</a><a href="https://github.com/MSR-LIT/XtremeDistil/">[code]</a> 
<li>Structure-Level Knowledge Distillation For
Multilingual Sequence Labeling<a href="https://www.aclweb.org/anthology/2020.acl-main.304.pdf">[paper]</a><a href="https://github.com/Alibaba-NLP/MultilangStructureKD">[code]</a> 
<li>Knowledge Distillation for
Multilingual Unsupervised Neural Machine Translation<a href="https://www.aclweb.org/anthology/2020.acl-main.324.pdf">[paper]</a>
<li>FastBERT: a Self-distilling BERT with Adaptive Inference Time<a href="https://www.aclweb.org/anthology/2020.acl-main.537.pdf">[paper]</a><a href="https://github.com/autoliuweijie/FastBERT">[code]</a>
<li>TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural
Language Processing<a href="https://www.aclweb.org/anthology/2020.acl-demos.2.pdf">[paper]</a><a href="https://github.com/airaria/TextBrewer">[code]</a>
<li>Distill, Adapt, Distill:
Training Small, In-Domain Models for Neural Machine Translation<a href="https://www.aclweb.org/anthology/2020.ngt-1.12.pdf">[paper]</a><a href="https://github.com/mitchellgordon95/kd-adapt">[code]</a>

</ul>

<strong>[CVPR 2020](https://openaccess.thecvf.com/CVPR2020_search)</strong>
 
 <ul>
 <li>Spatio-Temporal Graph for Video Captioning With Knowledge Distillation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>
 <li>Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yin_Dreaming_to_Distill_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Search to Distill: Pearls Are Everywhere but Not the Eyes <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Search_to_Distill_Pearls_Are_Everywhere_but_Not_the_Eyes_CVPR_2020_paper.pdf">[paper]</a>
 <li>Explaining Knowledge Distillation by Quantifying the Knowledge<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf">[paper]</a>
 <li>Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Networks_Are_More_Productive_Teachers_Than_Human_Raters_Active_CVPR_2020_paper.pdf">[paper]</a> <a href="https://www.youtube.com/watch?v=yBO8olcWHvE">[video]</a> 
 <li>Regularizing Class-Wise Predictions via Self-Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yun_Regularizing_Class-Wise_Predictions_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Creating_Something_From_Nothing_Unsupervised_Knowledge_Distillation_for_Cross-Modal_Hashing_CVPR_2020_paper.pdf">[paper]</a><a href="https://www.youtube.com/watch?v=Io1uloVOEJk">[video]</a> 
 <li>Heterogeneous Knowledge Distillation Using Information Flow Modeling<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Passalis_Heterogeneous_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Revisiting Knowledge Distillation via Label Smoothing Regularization<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yuan_Revisiting_Knowledge_Distillation_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Collaborative Distillation for Ultra-Resolution Universal Style Transfer<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.pdf">[paper]</a>
 <li>Few Sample Knowledge Distillation for Efficient Network Compression<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Few_Sample_Knowledge_CVPR_2020_supplemental.pdf">[supp]</a> 
 <li>Online Knowledge Distillation via Collaborative Learning<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf">[paper]</a>
 <li>Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Block-Wisely_Supervised_Neural_Architecture_Search_With_Knowledge_Distillation_CVPR_2020_paper.pdf">[paper]</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Li_Block-Wisely_Supervised_Neural_CVPR_2020_supplemental.pdf">[supp]</a> <a href="https://www.youtube.com/watch?v=DCBYhdCPpoA">[video]</a> 
 </ul>

 ## TODO


<!-- #SIGIR -->
 <strong>[SIGIR 2020](http://static.ijcai.org/2020-accepted_papers.html)</strong>
<ul>
<li>Differentially Private Knowledge Distillation for Mobile Analytics [short paper] <a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401259"> [page]</a>
<li>Distilling Knowledge for fast retrieval-based chat-bots [short paper]<a href="https://arxiv.org/pdf/2004.11045.pdf">[paper]</a>
<li>A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data <a href="http://csse.szu.edu.cn/staff/panwk/publications/Conference-SIGIR-20-KDCRec.pdf">[paper]</a>
<a href="https://github.com/dgliu/SIGIR20_KDCRec.">[code]</a>
</ul>


<!-- #IJCAI -->
 <strong>[IJCAI 2020](http://static.ijcai.org/2020-accepted_papers.html)</strong>
<ul>
<li>P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection <a href="https://www.ijcai.org/Proceedings/2020/0448.pdf">[paper]</a>
</ul>

<!-- #ECCV -->
 <strong>[ECCV 2020](https://eccv2020.eu/)</strong>
<ul>
<li>Circumventing Outliers of AutoAugment with Knowledge Distillation<a href="">[paper]</a>
<li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480613.pdf">[paper]</a>
<li>Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500239.pdf">[paper]</a>
<li>Knowledge Distillation Meets Self-Supervision<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540562-supp.pdf">[supp]</a>
<li>Local Correlation Consistency for Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018-supp.zip">[supp]</a>
<li>Robust Re-Identification by Multiple Views Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550103.pdf">[paper]</a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550103-supp.pdf">[supp]</a>
<li>AMLN: Adversarial-based Mutual Learning Network for Online Knowledge Distilla<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570154.pdf">[paper]</a>
<li>Differentiable Feature Aggregation Search for Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620460.pdf">[paper]</a>
<li>Online Ensemble Model Compression using Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640018.pdf">[paper]</a>
<li>Exclusivity-Consistency Regularized Knowledge Distillation for Face Recognition<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690324.pdf">[paper]</a>
<li>Feature Normalized Knowledge Distillation for Image Classification<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700664.pdf">[paper]</a>
<li>Weight Decay Scheduling and Knowledge Distillation for Active Learning<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710426.pdf">[paper]</a>
<li>Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710647.pdf">[paper]</a><a href="hhttps://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710647-supp.pdf">[supp]</a>
<li>Interpretable Foreground Object Search As Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730188.pdf">[paper]</a>
<li>Improving Knowledge Distillation via Category Structure<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730205.pdf">[paper]</a>
<li>Circumventing Outliers of AutoAugment with Knowledge Distillation<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480613.pdf">[paper]</a>
</ul>


<strong>[ICLR 2020](https://iclr.cc/virtual_2020/papers.html?filter=keywords&search=knowledge+distillation)</strong>

<ul>
<li>Neural Epitome Search for Architecture-Agnostic Network Compression <a href="https://openreview.net/pdf?id=HyxjOyrKvr">[paper]</a>
<li>Contrastive Representation Distillation<a href="http://www.openreview.net/pdf?id=SkgpBJrtvS">[paper]</a><a href="https://github.com/HobbitLong/RepDistiller">[code]</a>
<li>Ensemble Distribution Distillation<a href="http://www.openreview.net/pdf?id=BygSP6Vtvr">[paper]</a>
<li>Understanding Knowledge Distillation in Non-autoregressive Machine Translation<a href="http://www.openreview.net/pdf?id=BygFVAEKDH">[paper]</a>
<li>Neural Epitome Search for Architecture-Agnostic Network Compression<a href="http://www.openreview.net/pdf?id=HyxjOyrKvr">[paper]</a>
</ul>


<strong>[NIPS 2020](https://papers.nips.cc/paper/2020)</strong>
<ul>
<li>Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher <a href="https://papers.nips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Supplemental.zip">[supp]</a>
<li>Task-Oriented Feature Distillation <a href="https://papers.nips.cc/paper/2020/file/a96b65a721e561e1e3de768ac819ffbb-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/a96b65a721e561e1e3de768ac819ffbb-Supplemental.zip">[supp]</a>
<li>Kernel Based Progressive Distillation for Adder Neural Networks <a href="https://papers.nips.cc/paper/2020/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/912d2b1c7b2826caf99687388d2e8f7c-Supplemental.pdf">[supp]</a>
<li>Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection<a href="https://papers.nips.cc/paper/2020/file/c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/c3535febaff29fcb7c0d20cbe94391c7-Supplemental.pdf">[supp]</a>
<li>Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search<a href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Supplemental.pdf">[supp]</a>
<li>Self-Distillation Amplifies Regularization in Hilbert Space<a href="https://papers.nips.cc/paper/2020/file/2288f691b58edecadcc9a8691762b4fd-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/2288f691b58edecadcc9a8691762b4fd-Supplemental.zip">[supp]</a>
<li>MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers<a href="https://papers.nips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Supplemental.pdf">[supp]</a>
<li>Self-Distillation as Instance-Specific Label Smoothing<a href="https://papers.nips.cc/paper/2020/file/1731592aca5fb4d789c4119c65c10b4b-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/1731592aca5fb4d789c4119c65c10b4b-Supplemental.pdf">[supp]</a>
<li>Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space <a href="https://papers.nips.cc/paper/2020/file/91c77393975889bd08f301c9e13a44b7-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/91c77393975889bd08f301c9e13a44b7-Supplemental.pdf">[supp]</a>
<li>Distributed Distillation for On-Device Learning<a href="https://papers.nips.cc/paper/2020/file/fef6f971605336724b5e6c0c12dc2534-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/fef6f971605336724b5e6c0c12dc2534-Supplemental.pdf">[supp]</a>
<li>Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts<a href="https://papers.nips.cc/paper/2020/file/657b96f0592803e25a4f07166fff289a-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/657b96f0592803e25a4f07166fff289a-Supplemental.zip">[supp]</a>
<li>Ensemble Distillation for Robust Model Fusion in Federated Learning<a href="https://papers.nips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Supplemental.pdf">[supp]</a>
<li>Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation<a href="https://papers.nips.cc/paper/2020/file/62d75fb2e3075506e8837d8f55021ab1-Paper.pdf">[paper]</a><a href="https://papers.nips.cc/paper/2020/file/62d75fb2e3075506e8837d8f55021ab1-Supplemental.zip">[supp]</a>
</ul>


## Federated Learning
<strong>[IEEE ICKG 2020](http://ickg2020.bigke.org/)</strong>
<ul>
<li>Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9194566">[paper]</a>
</ul>

 <strong>[IEEE Signal Processing Magazine 2020](http://ickg2020.bigke.org/)</strong>
<ul>
<li>Federated Learning: Challenges, Methods, and Future Directions<a href="https://ieeexplore.ieee.org/abstract/document/9084352">[paper]</a>
</ul>

 <strong>[ACM TIST 2019](https://dl.acm.org/journal/tist)</strong>
<ul>
<li>Federated Machine Learning: Concept and Applications <a href="http://sites.nlsde.buaa.edu.cn/~yxtong/tist_fl.pdf">[paper]</a>
</ul>



### Applications:
<ul>
    <li>A review of applications in federated learning<a href="https://www.sciencedirect.com/science/article/pii/S0360835220305532">[paper]</a>
    <li>Federated learning in medicine:
    facilitating multi‑institutional
    collaborations without sharing
    patient data<a href="https://www.nature.com/articles/s41598-020-69250-1">[article]</a>
</ul>

<hr/>
<hr/>

# To-Do: Knowledge Distillation Survey Paper List

## Different form of Knowledge

### Response-Based Knowledge
<ul>
<li> <strong>Do deep nets really need to be deep?</strong> Ba, J., & Caruana, R. (2014). NIPS <a href="https://papers.nips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf">[paper]</a></li> 


<li> <strong>Distilling the knowledge in a neural network.</strong> Hinton, G., Vinyals, O., & Dean, J. (2015). <a href="http://www.cs.toronto.edu/~hinton/absps/distillation.pdf">[paper]</a></li>

<li><strong>Transferring
knowledge to smaller network with class-distance
loss.</strong> Kim, S. W. & Kim, H. E. (2017). In: ICLRW. <a href="https://openreview.net/pdf?id=ByXrfaGFe">[paper]</a></li>

<li><strong>Improved knowledge distillation via teacher assistant.</strong>Mirzadeh, S. I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., & Ghasemzadeh, H. (2020, April). AAAI. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5963">[paper]</a></li>

<li><strong>Learning efficient object detection models with knowledge distillation.</strong>Chen, G., Choi, W., Yu, X., Han, T., & Chandraker, M. (2017). In: NeurIPS. <a href="https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf">[paper]</a></li>

<li><strong>Conditional teacher-student learning.</strong>Meng, Z., Li, J., Zhao, Y., & Gong, Y. (2019, May). In: ICASSP. <a href="https://arxiv.org/pdf/1904.12399">[paper]</a></li>

<li><strong>Feature fusion for online mutual knowledge distillation.</strong>Kim, J., Hyun, M., Chung, I., & Kwak, N. (2019). In: ICPR. <a href="https://arxiv.org/pdf/1904.09058.pdf">[paper]</a></li>

<li><strong>When does label smoothing help?</strong>Müller, R., Kornblith, S., & Hinton, G. E. (2019). In: NeurIPS. <a href="https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf">[paper]</a></li>

</ul>

### Feature-Based Knowledge

<li><strong>WFitnets: Hints for thin deep nets.</strong>Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2014). In: ICLR. <a href="https://arxiv.org/pdf/1412.6550.pdf?source=post_page---------------------------">[paper]</a></li>

<li><strong>Paying more
attention to attention: Improving the performance of
convolutional neural networks via attention transfer.</strong>Zagoruyko, S., & Komodakis, N. (2016).  In: ICLR. <a href="https://arxiv.org/pdf/1612.03928.pdf">[paper]</a></li>

<li><strong>Spatiotemporal distilled dense-connectivity network for video action recognition.</strong>Hao, W., & Zhang, Z. (2019). Pattern Recognition. <a href="http://159.226.21.68/bitstream/173211/23347/1/Spatiotemporal%20distilled%20denseConnectivity%20network%20for%20video%20action%20recognition.pdf">[paper]</a></li>

<li><strong>Learning deep representations with probabilistic knowledge transfer.</strong>Passalis, N. & Tefas, A. (2018).  In: ECCV. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.pdf">[paper]</a></li>

<li><strong>Paraphrasing complex network: Network compression via factor transfer.</strong> Kim, J., Park, S., & Kwak, N. (2018). In: NeurIPS <a href="https://proceedings.neurips.cc/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation via
route constrained optimization. </strong>Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D.,
Yan, J. & Hu, X. (2019). In: ICCV. <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Jin_Knowledge_Distillation_via_Route_Constrained_Optimization_ICCV_2019_paper.pdf">[paper]</a></li>

<li><strong>Knowledge distillation with adversarial samples
supporting decision boundary.</strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4263/4141">[paper]</a></li>

<li><strong>Rocket launching: A universal and efficient framework for training well-performing light net.</strong>Zhou, G., Fan, Y., Cui, R., Bian, W., Zhu, X., & Gai, K. (2017).  <a href="https://arxiv.org/pdf/1708.04106">[paper]</a></li>

<li><strong>Differentiable Feature Aggregation Search for Knowledge Distillation.</strong>Guan, Y., Zhao, P., Wang, B., Zhang, Y., Yao, C., Bian, K., & Tang, J. (2020, August). In: ECCV <a href="https://arxiv.org/pdf/2008.00506">[paper]</a></li>

<li><strong> Amalgamating knowledge towards comprehensive classification</strong>Shen, C., Wang, X., Song, J., Sun, L., & Song, M.
(2019). In: AAAI. <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4165/4043">[paper]</a></li>

### Relation-Based Knowledge

<li><strong></strong>Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019) In: NeurIPS <a href="">[paper]</a></li>

### - Relationships between Intermediate layers

### - Parameters of teacher model

## Different mode of distillation

## KD algorithms

## KD Applications

<ul>
<li> Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN
</ul>



